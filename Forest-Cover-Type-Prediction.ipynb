{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6064d601-1f38-4ddd-8d66-9b7b555c218b",
   "metadata": {},
   "source": [
    "# Forest Cover Type Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de13e0-a8af-42c5-9d49-a8559878d384",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd862b3-5eca-4dbd-b452-a6d8641b1d3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f70e55-536f-43bb-98d0-ae5ca70d43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "NUM_FOLDS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8bf1e5-2648-460d-9c3d-ad28364b78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy\n",
    "import time\n",
    "import gc\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Model & Evaluation\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, Normalizer, PowerTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics, ensemble,linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46396b06-380f-4c3b-8e2d-3fab67ee061d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e362ff-3d9c-44fa-ba36-2f3dae464228",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17bc792-f665-4ad2-8466-82b4e8a77c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([test.assign(indic=\"test\"), train.assign(indic=\"train\")]).set_index(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57745d5e-e572-405d-a32e-e9c54d3d0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309745f-841b-49e3-9a6d-4a067dfdf223",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Exploratory Data Analysis (descriptive analytics) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae33c6-39ac-497b-ae2e-93c2ddbd0d5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Basic understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1e17f-00d6-4048-879e-64baa477097b",
   "metadata": {},
   "source": [
    "Due to our little understanding of trees and nature, we decided not to use any automated exploratory data analysis tool for data exploration. We believe that a manual approach would better understand the variables and their relationship between them and the target. \n",
    "\n",
    "In this very first step, we can see that all our variables are numerical and maybe, most of them are already One-Hot-Encoded. Furthermore, we ca see that there are 561012 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db2c8d-6202-476e-8b11-c5f198c74371",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdecee3-6cf9-4cfa-a82f-b6224dcb2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ccf44-60f4-43ea-b9e4-9a1d5df0b776",
   "metadata": {},
   "source": [
    "After running a statistical analysis, we can see that vertical distance to hydrology have negatives values, which means that we can't use some tools such as the chi-square test and log transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e1a41-bd32-47f1-a603-01b8c378f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ac3c0-afa1-4cc9-a8e7-bdd2f7f3fa01",
   "metadata": {},
   "source": [
    "We can see that there are columns with just two unique values, which means that there are categorical values. We will check it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26418ffb-f4cc-4249-bc7c-043961ce851a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c65322-aefb-41a0-8456-cd7ae81f2fe9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Null values in the data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fc9cb-f61d-4293-8de1-d7b3b9df7ceb",
   "metadata": {},
   "source": [
    "There are no values in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b101c-9086-41c2-abe3-7828cec4309d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4fc3b8-d05b-4376-a9e5-63bc1d5174aa",
   "metadata": {},
   "source": [
    "### 2.3 Number of zeros in all columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb870eee-3cd6-424b-a0e0-4668b519b33d",
   "metadata": {},
   "source": [
    "There are several zero values in the dataset. For some features, it could be expected. Aspect is the degrees can be 0 and some measures such as horizontals and vertical distances. However, even if zero values can be possible, further exploration of the dataset and a more profound investigation of the topic, to be sure that those zero values are not errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d797e3-a3cf-4ac9-8920-0d97cc1b5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in df.iloc[:,0:11]:\n",
    "    column = df[variable] \n",
    "    count = (column == 0).sum()\n",
    "    print('number of zeros in column ', variable, ' is:',count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f562cda-dfea-4b64-a7f3-7ed05be95b87",
   "metadata": {},
   "source": [
    "### 2.4 Checking if binary variables are really binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74cd8f9-74e3-4bd6-ab85-cb15c7bfa9d0",
   "metadata": {},
   "source": [
    "We can see that the features with two values are one-hot encoded features. All of them are 0 or 1. However, soil_7 has only 105 values, soil_15 has 3 and soil_36 only 119."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb705686-37b9-41ef-b8f0-2443a90ec2c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in df.iloc[:,10:55]:\n",
    "    print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36ecac-ae5a-48d8-b5d4-782f1cc012bc",
   "metadata": {},
   "source": [
    "### 2.5 Analysing continous numerical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d26ce8-85a8-4650-8ad0-1622674e2d90",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Checking for Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d69d2-ea9b-435c-a72a-bf19cd6c49b1",
   "metadata": {},
   "source": [
    "##### Finding outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fe7a7-9b63-4b26-841f-f98d2ce8998e",
   "metadata": {},
   "source": [
    "We can see that our dataset has several outliers. Sometimes, it can negatively affect our data cleaning or our prediction. However, the presence of outliers can also tell us a lot about our data's behavior and help us to understand our work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5b9f1-600d-47bf-902b-456d39a40d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_IQR(df):\n",
    "    q1=df.quantile(0.25)\n",
    "    q3=df.quantile(0.75)\n",
    "    IQR=q3-q1\n",
    "    outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44124e42-7845-4bfc-b7e1-c8191c77f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_outliers_IQR(df.iloc[:,1:11]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e5924-66b5-4d61-8ee8-3f5e66837149",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.iloc[:,1:11]:\n",
    "    plt.figure()\n",
    "    df.boxplot([column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3e616-ec65-4938-b84c-d23efb4909aa",
   "metadata": {},
   "source": [
    "### 2.6 Variables Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371efa8a-4e30-4194-a1a5-fd275fb934d4",
   "metadata": {},
   "source": [
    "#### Distributon of the Predicted variable\n",
    "\n",
    "Looking at this graph, we can see that we have almost the same quantity of values for every type of cover forest, which is the target variable. Having a balanced distribution between the categories helps our models to have a better prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0bab7-b7da-4482-a375-af47da8e47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(10,6)})\n",
    "\n",
    "sns.histplot(df[\"Cover_Type\"], kde=False, color ='green')\n",
    "\n",
    "plt.xlabel(\"Cover_Type\")\n",
    "plt.ylabel(\"Number\")\n",
    "plt.title(\"Distribution of Cover Types\")\n",
    "plt.legend([\"Count\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811061c-1bbe-4f60-a6cb-fab3f4b7d8f7",
   "metadata": {},
   "source": [
    "### 2.7 Correlation between numerical variables\n",
    "\n",
    "Looking at the correlation matrix, we can see two correlated variables,  hillshade 9 am and hillshade 3 pm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5765315-14e9-482d-8c23-1b71027ba73e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,7))\n",
    "sns.heatmap(df.iloc[:,0:10].corr(), annot=True, cmap = 'flare');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9dbb97-9f91-424c-8f63-89ad2d61429b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.8 Undoing the one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe32cf-c646-41b1-a3d7-d1c5aeafffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilderness_area_encoding(df):\n",
    "    data = df.copy()\n",
    "    data['Wilderness_Area'] = 0\n",
    "    for i in range(1,5):\n",
    "        data['Wilderness_Area'] += i*data[f'Wilderness_Area{i}']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5fa49-1b73-405d-887f-29c8b9d71cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soil_type_encoding(df):\n",
    "    data = df.copy()\n",
    "    data['Soil_Type'] = 0\n",
    "    for i in range(1,41):\n",
    "        data['Soil_Type'] += i*data[f'Soil_Type{i}']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032c145-12ca-4cd9-907d-2e46d982c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= soil_type_encoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d89fd4-328c-421b-8cd8-524be3bb3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wilderness_area_encoding(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f609e98-ca61-4365-ab59-cebc0a9057af",
   "metadata": {},
   "source": [
    "###  Number of covert_type by wilderness_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754d642-b9fc-4e1c-b13e-5f923c35d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,7))\n",
    "sns.countplot(x = 'Wilderness_Area', hue = 'Cover_Type', data = df,palette = \"Greens\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ad0b0-7e6a-430d-bdf7-2d15b64fdfbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcaa5684-7e6d-4fee-9aef-48cfc19d1d23",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.9 Numerical Continious Variables \n",
    "We thought it would be interesting to know the relationship betweent the numerical continuious variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1c03d-7f83-4c3d-84e2-2774d653d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that elevation is the most important feature in our dataset, followed by vertical and horizontal distance to hydrology and fire points. It could be interesting to know the relationships of those variables. \n",
    "fg, ax = plt.subplots(nrows=2, ncols=2,figsize=(15,10), sharex=True,)\n",
    "sns.scatterplot(data=df, x=\"Elevation\", y=\"Horizontal_Distance_To_Roadways\",  color= 'green', ax=ax[0,0])\n",
    "sns.scatterplot(data=df, x=\"Elevation\", y=\"Horizontal_Distance_To_Fire_Points\",  color= 'green', ax=ax[0,1])\n",
    "sns.scatterplot(data=df, x=\"Elevation\", y=\"Horizontal_Distance_To_Hydrology\",  color= 'green', ax=ax[1,0])\n",
    "sns.scatterplot(data=df, x=\"Elevation\", y=\"Vertical_Distance_To_Hydrology\",  color= 'green', ax=ax[1,1]);  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03f48d-17aa-4d2f-84f6-825e9e612210",
   "metadata": {},
   "source": [
    "As we can see there is a linear relationship between this variables. That can help us to come up with new variables that can help to improve the model's accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d4940-5a66-4853-8ff3-ba4c44e13cee",
   "metadata": {},
   "source": [
    "### 2.10 Distribution of Variables \n",
    "\n",
    "Continous variables are not normaly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a1429-2d02-433c-9bd0-44b9f0ff6a4f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in df.iloc[:,1:10]:\n",
    "    #sns.displot(df[i], label= i)\n",
    "    sns.displot(df[i], label= i, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d342a-a325-4f2e-b212-3fb66564529e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f770599-e5a6-4bd1-9f9f-f760496cc441",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.1 Scaling\n",
    "We will be scaling it the pipeline \n",
    "\n",
    "We will try Robust and Standard scaler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65f6b9-3336-4b56-a59b-b9e113f76cb0",
   "metadata": {},
   "source": [
    "### 3.2 Normalising\n",
    "We will be normalising in the pipeline\n",
    "\n",
    "We will try PowerTransformer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2031ed6-a3ff-405c-bb0f-46d2de7e8c5b",
   "metadata": {},
   "source": [
    "### 3.3 Outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e28d5e-0ea9-4d7d-ae8f-0dc1ae75d791",
   "metadata": {},
   "source": [
    "We decided to keep them since we don't have enough knowledge about the data and what is actually considered an outlier or not "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811d537-71d1-490f-8eea-02688f59137d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6756ee-af03-42a8-bca7-42564a1d5b1b",
   "metadata": {},
   "source": [
    "### 4.1 Add General Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641c863-d760-49e8-bef1-b8158d052df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def add_general_features(data):\n",
    "    df = data.copy()\n",
    "\n",
    "            \n",
    "    df['Horizontal_Distance_To_Roadways_Log'] = [math.log(v+1) for v in df['Horizontal_Distance_To_Roadways']]\n",
    "    df['Water Elevation'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n",
    "    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n",
    "    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n",
    "    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n",
    "    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n",
    "    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n",
    "    df['Elev_3Horiz'] = df['Elevation'] + df['Horizontal_Distance_To_Roadways']  + df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Hydrology']\n",
    "    df['Elev_Road_1'] = df['Elevation'] + df['Horizontal_Distance_To_Roadways']\n",
    "    df['Elev_Road_2'] = df['Elevation'] - df['Horizontal_Distance_To_Roadways']\n",
    "    df['Elev_Fire_1'] = df['Elevation'] + df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Elev_Fire_2'] = df['Elevation'] - df['Horizontal_Distance_To_Fire_Points']\n",
    "    \n",
    "    # Fill NA\n",
    "    df.fillna(0, inplace = True)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46a098-8702-4972-9d1f-cbd3f810e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_general_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52d878-5cc2-4777-bfac-f75380dcc6b2",
   "metadata": {},
   "source": [
    "### 4.2 Soil Type Features\n",
    "new features based on the soil-type variables:\n",
    "\n",
    "\n",
    "1. Climatic Zone\n",
    "2. Geologic Zone\n",
    "3. Surface Cover\n",
    "4. Rock Size\n",
    "5. Interaction Terms\n",
    "\n",
    "\n",
    "### ELU Codes\n",
    "The soil type number is based on the USFS Ecological Landtype Units (ELUs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178fdbe0-7417-4a14-91d2-0aac995f62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELU_CODE = {\n",
    "    1:2702,2:2703,3:2704,4:2705,5:2706,6:2717,7:3501,8:3502,9:4201,\n",
    "    10:4703,11:4704,12:4744,13:4758,14:5101,15:5151,16:6101,17:6102,\n",
    "    18:6731,19:7101,20:7102,21:7103,22:7201,23:7202,24:7700,25:7701,\n",
    "    26:7702,27:7709,28:7710,29:7745,30:7746,31:7755,32:7756,33:7757,\n",
    "    34:7790,35:8703,36:8707,37:8708,38:8771,39:8772,40:8776\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6697749-41fa-47fb-86bc-d3dcb70806c4",
   "metadata": {},
   "source": [
    "### 4.2.1  Climatic Zone (Ordinal Variable)\n",
    "\n",
    "1. lower montane dry\n",
    "2. lower montane\n",
    "3. montane dry\n",
    "4. montane\n",
    "5. montane dry and montane\n",
    "6. montane and subalpine\n",
    "7. subalpine\n",
    "8. alpine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274b513-0e31-4ab6-965b-727592b47eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climatic_zone(df):\n",
    "    data = df.copy()\n",
    "    data['Climatic_Zone'] = df['Soil_Type'].apply(\n",
    "        lambda x: int(str(ELU_CODE[x])[0])\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb77d7c-5530-4ec9-a07e-095e6c8cb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = climatic_zone(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e82ab-5180-47f1-b791-424ef5149527",
   "metadata": {},
   "source": [
    "### 4.2.2  Geologic Zone (Nominal Variable)\n",
    "\n",
    "1. alluvium\n",
    "2. glacial\n",
    "3. shale\n",
    "4. sandstone\n",
    "5. mixed sedimentary\n",
    "6. unspecified in the USFS ELU Survey\n",
    "7. igneous and metamorphic\n",
    "8. volcanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc46b86-19da-4f11-a895-c68db75dac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geologic_zone(df):\n",
    "    data = df.copy()\n",
    "    data['Geologic_Zone'] = df['Soil_Type'].apply(\n",
    "        lambda x: int(str(ELU_CODE[x])[1])\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef2b72-7b7f-47d8-8358-2e72889467f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geologic Zone\n",
    "df = geologic_zone(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513f37a-7096-435f-86e0-94eef74b6172",
   "metadata": {},
   "source": [
    "### 4.2.3 Surface Cover (Ordinal Variable)\n",
    "\n",
    "1. (Stony/Bouldery) \n",
    "2. (Very Stony/Very Bouldery) \n",
    "3. (Extremely Stony/Extremely Bouldery) \n",
    "4. (Rubbly)\n",
    "5. (Very Rubbly) \n",
    "\n",
    "If no description of the surface cover is given, we give it a value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa666c-5f5c-412e-a99a-32f1d51536f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface_cover(df):\n",
    "    #Group IDs\n",
    "    no_desc = [7,8,14,15,16,17,19,20,21,23,35]\n",
    "    stony = [6,12]\n",
    "    very_stony = [2,9,18,26]\n",
    "    extremely_stony = [1,22,24,25,27,28,29,30,31,32,33,34,36,37,38,39,40]\n",
    "    rubbly = [3,4,5,10,11,13]\n",
    "\n",
    "    #Create dictionary\n",
    "    surface_cover = {i:0 for i in no_desc}\n",
    "    surface_cover.update({i:1 for i in stony})\n",
    "    surface_cover.update({i:2 for i in very_stony})\n",
    "    surface_cover.update({i:3 for i in extremely_stony})\n",
    "    surface_cover.update({i:4 for i in rubbly})\n",
    "    \n",
    "    #Create Feature\n",
    "    data = df.copy()\n",
    "    data['Surface_Cover'] = df['Soil_Type'].apply(\n",
    "        lambda x: surface_cover[x]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb807e0-c37c-4085-b2cf-d39f08d9800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Surface cover\n",
    "df = surface_cover(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bc35f-bf8c-4f8c-96d0-ca1fe049a049",
   "metadata": {},
   "source": [
    "### 4.2.4 Rock Size (Nominal)\n",
    "\n",
    "1. Stones\n",
    "2. Boulders\n",
    "3. Rubble\n",
    "\n",
    "If the soil type description has no mention of rock size, we give it a default value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03bae0-73f0-4ca7-9e08-3cbc1a441fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rock_size(df):\n",
    "    \n",
    "    # Group IDs\n",
    "    no_desc = [7,8,14,15,16,17,19,20,21,23,35]\n",
    "    stones = [1,2,6,9,12,18,24,25,26,27,28,29,30,31,32,33,34,36,37,38,39,40]\n",
    "    boulders = [22]\n",
    "    rubble = [3,4,5,10,11,13]\n",
    "\n",
    "    # Create dictionary\n",
    "    rock_size = {i:0 for i in no_desc}\n",
    "    rock_size.update({i:1 for i in stones})\n",
    "    rock_size.update({i:2 for i in boulders})\n",
    "    rock_size.update({i:3 for i in rubble})\n",
    "    \n",
    "    data = df.copy()\n",
    "    data['Rock_Size'] = df['Soil_Type'].apply(\n",
    "        lambda x: rock_size[x]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a78451-7088-4a53-866f-13e1d40d89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rock size\n",
    "df = rock_size(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d2ae4-52a8-4d5a-b3e8-6429cd309934",
   "metadata": {},
   "source": [
    "### 4.2.5 Soil Type Interactions\n",
    "We include only those features which resulted in improved CV accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160f511-73cc-44b4-b352-e54f5f9f3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soiltype_interactions(data):\n",
    "    df = data.copy()\n",
    "            \n",
    "    # Important Soil Types\n",
    "    df['Soil_12_32'] = df['Soil_Type32'] + df['Soil_Type12']\n",
    "    df['Soil_Type23_22_32_33'] = df['Soil_Type23'] + df['Soil_Type22'] + df['Soil_Type32'] + df['Soil_Type33']\n",
    "    \n",
    "    # Soil Type Interactions\n",
    "    df['Soil29_Area1'] = df['Soil_Type29'] + df['Wilderness_Area1']\n",
    "    df['Soil3_Area4'] = df['Wilderness_Area4'] + df['Soil_Type3']\n",
    "    \n",
    "    #  New Feature Interactions\n",
    "    df['Climate_Area2'] = df['Wilderness_Area2']*df['Climatic_Zone'] \n",
    "    df['Climate_Area4'] = df['Wilderness_Area4']*df['Climatic_Zone'] \n",
    "    df['Rock_Area1'] = df['Wilderness_Area1']*df['Rock_Size']    \n",
    "    df['Rock_Area3'] = df['Wilderness_Area3']*df['Rock_Size']  \n",
    "    df['Surface_Area1'] = df['Wilderness_Area1']*df['Surface_Cover'] \n",
    "    df['Surface_Area2'] = df['Wilderness_Area2']*df['Surface_Cover']   \n",
    "    df['Surface_Area4'] = df['Wilderness_Area4']*df['Surface_Cover'] \n",
    "    \n",
    "    # Fill NA\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d681ba6-d36e-4368-a196-df5803e47c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soil type interactions\n",
    "df = soiltype_interactions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d322d4-2350-49d7-b81b-dd56bf258118",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(40,30))\n",
    "sns.heatmap(df.corr(), annot=True, cmap = 'flare');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827205d-5e0b-4dd4-815c-c1ae698b2a85",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3 Dimension Reduction\n",
    "We drop the original soil features since the new features discribe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2f763-71de-413b-81c6-bb06a6ffd8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_features = [f'Soil_Type{i}' for i in range(1,41)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125eb96-5144-43b5-870f-cde65c14b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = soil_features, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cde208-9dff-4556-b25a-13c7394d687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilderness_features = [f'Wilderness_Area{i}' for i in range(1,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503881b-9526-48eb-a4e7-826347c40888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = wilderness_features, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14595e5f-eb51-43a1-8fec-56b4cd6c98a7",
   "metadata": {},
   "source": [
    "### Correlations after feautre engineering  \n",
    "There are still many correlated features! but some algorithms can help to deal with this problem.\n",
    "Since in the real world, we deal with a big quantity of feauters that can't be picked manually, we wanted to test all those features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89ae1d-35d9-45b4-9cf7-f98cee7dc7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(30,30))\n",
    "sns.heatmap(df.corr(), annot=True, cmap = 'flare');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c25de-bbc4-4e5e-aaa0-4eb3fbd8accd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e83526-941d-4396-9826-f70d02c911c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1 Split dataframe \n",
    "Here we are splitting our dataframe back to the train and test based on the ind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf2aeba-4603-47e5-af67-733326fdbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = df[df[\"indic\"].eq(\"test\")],df[df[\"indic\"].eq(\"train\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acded4fe-c5ba-43ce-929c-83c6bb3a6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['Cover_Type','indic'],axis=1)\n",
    "X_test = test.drop(['Cover_Type', 'indic'],axis=1)\n",
    "y = train.Cover_Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f843bc7-fe17-469d-a500-2e59aa903aa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.2 Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7627255-ea64-49b7-89a0-3646749bf723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the most important features of a logreg classifier based on the coefficient values\n",
    "def get_feature_importance(clf, feature_names):\n",
    "    return pd.DataFrame({'variable': feature_names, # Feature names\n",
    "                         'coefficient': clf.coef_[0] # Feature Coeficients\n",
    "                    }) \\\n",
    "    .round(decimals=2) \\\n",
    "    .sort_values('coefficient', ascending=False) \\\n",
    "    .style.bar(color=['red', 'green'], align='zero')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33c9dd-9865-4844-966f-2dd8b9f4670c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.2.1 Ridge Reguralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77c399-d0b4-4b26-ba37-2cc7019fbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('scaler', RobustScaler()), ('model', Ridge(alpha=1.0))])\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {}\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 12, shuffle = True, random_state = 0)\n",
    "\n",
    "# Create a grid search to try all the possible number of PCs\n",
    "estimator = GridSearchCV(pipeline, param_grid, cv=skf)\n",
    "estimator.fit(X, y)\n",
    "estimator.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af5cd8-07e8-4a20-8ab4-9040e0c5beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_mod = linear_model.LogisticRegression(max_iter=10000,penalty='l2')\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(ridge_mod, X, y, cv=skf))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b1273-15da-440f-a036-82686ddcc5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importance(ridge_mod.fit(X,y), X.columns.get_level_values(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb763900-d00d-45c0-8cf2-d235c298462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_mod = linear_model.LogisticRegression(max_iter=10000,penalty='l2')\n",
    "alphas = 10**np.linspace(-1,-4,100)\n",
    "\n",
    "coefs_ = [] \n",
    "scores_ = [] \n",
    "\n",
    "# Go over the regularization values list defined above, train a logreg model for each of the regularization values and evaluate it.\n",
    "for a in alphas:\n",
    "    ridge_mod.set_params(C=a) # Set the regularization parameter \n",
    "    scores_.append(np.mean(cross_val_score(ridge_mod, X, y, cv=skf))) # Appends the accuracy of the model\n",
    "    coefs_.append(ridge_mod.fit(X, y).coef_.ravel().copy()) # Appends the coefficient of the model\n",
    "\n",
    "# Conver the coefficient and scores arrays to numpy arrays\n",
    "coefs_ = np.array(coefs_)\n",
    "scores_ = np.array(scores_)\n",
    "\n",
    "# Define the figures to plot the values\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "fig.suptitle('Logistic Regression Path', fontsize=20)\n",
    "\n",
    "# Coeff Weights Plot\n",
    "ax1.plot(alphas, coefs_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax1.set_ylabel('Coefficient Weights', fontsize = 15)\n",
    "ax1.set_xlabel('Alpha', fontsize = 15)\n",
    "ax1.axis('tight')\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(alphas, scores_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax2.set_ylabel('Accuracy Score', fontsize = 15)\n",
    "ax2.set_xlabel('Alpha', fontsize = 15)\n",
    "ax2.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e96f1e-ae8a-472e-982f-6ec2efd0cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "alphas = 10**np.linspace(-1,-4,100)\n",
    "\n",
    "ridge_mod_cv = linear_model.LogisticRegressionCV(max_iter=10000,penalty='l2',Cs=alphas)\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(ridge_mod_cv, X, y, cv=skf))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6aa9d2-8412-474b-99cf-ac342b004724",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  5.2.2 Lasso Reguralized\n",
    "Ridge gave us a score of 0.3467. We Will see if Lasso (which actually removes features by making their coefficients equal to 0) improves the unregularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c7db5-c4b3-42fc-97e3-d92d0a61d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('scaler', RobustScaler()), ('normaliser', PowerTransformer()), ('model', linear_model.LogisticRegression())])\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {'model__penalty':['l1'], 'model__solver':['liblinear']}\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 12, shuffle = True, random_state = 0)\n",
    "\n",
    "# Create a grid search to try all the possible number of PCs\n",
    "estimator = GridSearchCV(pipeline, param_grid, cv=skf)\n",
    "estimator.fit(X, y)\n",
    "estimator.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dfd94-b1e0-4c85-8f3b-c346dbc5d4d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lasso_mod = linear_model.LogisticRegression(penalty='l1', solver='liblinear')\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(lasso_mod, X, y, cv=skf))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494807bd-17d7-43a4-be60-af24217f5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importance(lasso_mod.fit(X,y), X.columns.get_level_values(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d6a67-5a36-4a28-bdf6-dd05e6ab2612",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lasso_mod = linear_model.LogisticRegression(penalty='l1',solver='liblinear')\n",
    "alphas = 10**np.linspace(-1,-4,100)\n",
    "\n",
    "coefs_ = []\n",
    "scores_ = []\n",
    "for a in alphas:\n",
    "    lasso_mod.set_params(C=a)\n",
    "    scores_.append(np.mean(cross_val_score(lasso_mod, X, y, cv=5))) # Appends the accuracy of the model\n",
    "    coefs_.append(lasso_mod.fit(X, y).coef_.ravel().copy()) # Appends the coefficient of the model\n",
    "\n",
    "coefs_ = np.array(coefs_)\n",
    "scores_ = np.array(scores_)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "fig.suptitle('Logistic Regression Path', fontsize=20)\n",
    "\n",
    "# Coeff Weights Plot\n",
    "ax1.plot(alphas, coefs_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax1.set_ylabel('Coefficient Weights', fontsize = 15)\n",
    "ax1.set_xlabel('log(C)', fontsize = 15)\n",
    "ax1.axis('tight')\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(alphas, scores_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax2.set_ylabel('Accuracy Score', fontsize = 15)\n",
    "ax2.set_xlabel('log(C)', fontsize = 15)\n",
    "ax2.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497b1dd-7746-41c2-8775-e8490d616cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lasso_mod_cv = linear_model.LogisticRegressionCV(max_iter=10000,penalty='l1',solver='liblinear',Cs=alphas)\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(lasso_mod_cv, X, y, cv=skf))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ed969-defd-456a-b674-84e0ddde3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_mod_cv.fit(X,y)\n",
    "model = SelectFromModel(X, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(\"Original Number of Features = {} --> Number of features selected by Lasso = {}\".format(X.shape[1], X_new.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b3ff7-bfd4-4a68-9b02-818667ab922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_lasso_mod = linear_model.LogisticRegression(max_iter=10000,penalty='l1', solver='liblinear')\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(reduced_lasso_mod, X_new, y, cv=5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906ae09-0ac2-49b7-9359-897a36277b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importance(reduced_lasso_mod.fit(X_new,y), X.columns[model.get_support()].get_level_values(0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243a25a-cc90-47cd-b88a-20111c1e5df8",
   "metadata": {},
   "source": [
    "Lasso gave us a score of 0.6909! better than ridge and with less features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbc4eb-6674-4b1b-937b-8b981a4ab2de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.3 Tree Based Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e2043a-0aca-4b82-a19f-0a24800560fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 5.3.1 Gradient Boosting\n",
    "- uses regulaiazed linear models \n",
    "- stores data on a data on a data structure called DMatrix for faster iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd5f4b-3127-4dea-94a9-7b574b22cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "pipeline = Pipeline(steps=[('scaler', RobustScaler()), ('model', GradientBoostingRegressor())])\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {'model__max_features':[20,23,25], 'model__min_samples_leaf':[1,2], 'model__n_estimators':[500,1000]}\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 12, shuffle = True, random_state = 0)\n",
    "\n",
    "# Create a grid search to try all the possible number of PCs\n",
    "estimator = GridSearchCV(pipeline, param_grid, cv=skf)\n",
    "estimator.fit(X, y)\n",
    "estimator.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b88ce-aafb-4890-ba62-8a3fda597ba6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a640fbb-94f4-40e9-9bd3-242a78290e7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('scaler', RobustScaler()), ('model', RandomForestClassifier())])\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {'model__n_estimators':[200], 'model__n_jobs':[-1]}\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 12, shuffle = True, random_state = 0)\n",
    "\n",
    "# Create a grid search to try all the possible number of PCs\n",
    "estimator = GridSearchCV(pipeline, param_grid, cv=skf)\n",
    "estimator.fit(X, y)\n",
    "estimator.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8d9f2-b964-4aca-9a26-2e12fef29f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking code from the class practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1abd2b-af3a-44e7-a2a7-b71a891b7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_bagging = RandomForestRegressor(random_state=42, max_features=len(X.columns))\n",
    "print(\"Accuracy = {0:.4f}\".format(-np.mean(cross_val_score(boston_bagging, X, y, scoring='accuracy'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef0af8-717e-4f0a-9484-c70a81770539",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "boston_bagging.fit(X,y)\n",
    "plt.bar(X.columns, boston_bagging.feature_importances_)\n",
    "plt.title('Feature Importance', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183503a-87ee-4af2-9d4c-bf38a0147f8d",
   "metadata": {},
   "source": [
    "The performance is similar with way less features! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98afb0a-2f5a-4276-aec7-194a13746444",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_rf = RandomForestRegressor(random_state=42, max_features='sqrt')\n",
    "print(\"MSE = {0:.4f}\".format(-np.mean(cross_val_score(boston_rf, X, y, scoring='neg_mean_squared_error'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab236dbd-f8c2-441b-99f0-5b5b7bd662f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "boston_rf.fit(X,y)\n",
    "plt.bar(X.columns, boston_rf.feature_importances_)\n",
    "plt.title('Feature Importance', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd3615-34dc-4636-87c5-b70ce93c4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final based on our results we chose the following important features features \n",
    "boston_rf_small = RandomForestRegressor(random_state=42, max_features='sqrt')\n",
    "print(\"MSE = {0:.4f}\".format(-np.mean(cross_val_score(boston_rf, X[['Elevation', 'Soil_Type','Water_Elevation', 'EVDtHb', 'EVDtH', 'EHDtH', 'Climatic_Zone']], y, scoring='neg_mean_squared_error'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7006b2-c51f-4a0b-9bcf-c2eaebe036e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.3.3 Extremely Randomized Trees (Extra Trees)\n",
    "two main differences with other tree based sembsle methods are:\n",
    "1. it splits nodes b y choosing cut-points fully at random \n",
    "2. it uses the whole learning sample (rather thana bootstrap replica) to grow the trees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffd6ec-3e2c-4e41-bf66-665d3c2f16fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Testing on our raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e929db-6153-41c1-9680-fd169a0f142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.read_csv('data/train.csv').drop(['Cover_Type'],axis=1)\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "y1 = train.Cover_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a460b8c-7cf0-45f6-9c83-8a64c83447b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline the entire process: Scale the data -> ExtraTreesClassifier\n",
    "pipeline = Pipeline(steps=[('scaler', RobustScaler()), ('model', ExtraTreesClassifier())])\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {'model__n_estimators': [116],'model__min_samples_split':[2], \n",
    "             'model__max_features':[14],'model__random_state':[42], 'model__n_jobs': [-1]}\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 12, shuffle = True, random_state = 0)\n",
    "\n",
    "# Create a grid search to try all the possible number of PCs\n",
    "estimator = GridSearchCV(pipeline, param_grid, cv=skf)\n",
    "estimator.fit(X1, y1)\n",
    "print(estimator.best_params_)\n",
    "print(estimator.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d94dc-a5ce-48b1-9bc0-91f4d3a08b62",
   "metadata": {},
   "source": [
    "##### Testing on our cleaned, & featured engineered data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c135f22-ac54-4dd0-959a-0d078c991eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline the entire process: Scale the data -> ExtraTreesClassifier\n",
    "pipeline = Pipeline(steps=[('scaler', RobustScaler()), ('model', ExtraTreesClassifier())])\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {'model__n_estimators': [115,116,117,118],'model__min_samples_split':[2], \n",
    "             'model__max_features':[14,15,16],'model__random_state':[0,42], 'model__n_jobs': [-1]}\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 12, shuffle = True, random_state = 0)\n",
    "\n",
    "# Create a grid search to try all the possible number of PCs\n",
    "estimator = GridSearchCV(pipeline, param_grid, cv=skf)\n",
    "estimator.fit(X, y)\n",
    "print(estimator.best_params_)\n",
    "print(estimator.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9fc81a-cc30-48d2-8740-82473852978b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.4 Saving to CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35254b-e917-46ba-a5ba-83488c4fa49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission with feature engineering\n",
    "submission['Cover_Type'] = estimator.best_estimator_.predict(X_test).astype(\"int\")\n",
    "submission.to_csv('GroupH_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d220d29-8d6d-4621-8ee6-842fdf62c276",
   "metadata": {},
   "source": [
    "ExtraTreesClassifier Accuracy: 0.9124\n",
    "\n",
    "Kaggle Accuracy: 0.81942\n",
    "\n",
    "Kaggle Account: Tamara Samaha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9917e-2439-45f1-88ab-91e5ba3394d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
